# django-azure-auth-ms-fabric
Running Spark using Apache Livy/Microsoft Fabric Livy endpoint. Based on Django authentication and using django-azure-auth

## Requirements
- Create an EntraID application and apply the following settings
    - On the **Manage/Authentication section**
        - Add a redirect URL in the **Web** section: http://localhost:5000/azure_auth/callback (use another endpoint for other environments)
    - On the **Certificates & secrets** section
        - Create a **client secret** and store the value in the .env file (CLIENT_SECRET)
    - On the **Token configuration** section
        - Add Optional Claim
            - Token type: ID
            - Claim: upn
        - Add Group Claim (ID, Access, SAML)
            - Group ID
            - Enable: Emit groups as role claims
    - On the **API permissions** section
        - Add permission (with grant admin consent) for
            - **Microsoft Graph** (email, GroupMember.Read.All, profile, User.Read, User.ReadBasic.All)
            - **Power BI Service** (Code.AccessAzureDataExplorer.All, Code.AccessAzureDataLake.All, Code.AccessAzureKeyvault.All, Code.AccessFabric.All, Code.AccessStorage.All, Item.ReadWrite.All, Lakehouse.Execute.All, Workspace.ReadWrite.All)
- Adjust .env file for other values
    - **DJANGO_SECRET** = Random value generated by Django framework when starting a project
    - **TENANT_ID** = Your Azure tenant ID
    - **CLIENT_ID** = Your EntraID application Client ID
    - **CLIENT_SECRET** = Your EntraID application secret
    - **REDIRECT_URI** = "http://localhost:5000/azure_auth/callback" for local testing. Use another endpoint for other environments
    - **LOGOUT_URI** = "http://localhost:5000/logout" for local testing. Use another endpoint for other environments
    - **ROLES** = '{"My_Admin_Entra_Group_ObjectID": "Administrators", "My_Editors_Entra_Group_ObjectID": "Editors", "My_Viewers_Entra_Group_ObjectID": "Viewers"}' This will map the groups you defined on EntraID side with the groups you create in Django admin
    - **GRAPH_USER_ENDPOINT** = "https://graph.microsoft.com/v1.0/me"
    - **GRAPH_MEMBER_ENDPOINT** = "https://graph.microsoft.com/v1.0/me/memberOf"
    - **LIVY_BACKEND**: Possible values "apache" or "fabric"
    - **LIVY_BASE_ENDPOINT** = "https://api.fabric.microsoft.com/v1/workspaces/MyWorkSpaceID/lakehouses/MyLakeHouseID/livyapi/versions/2023-12-01". Replace MyWorkSpaceID and MyLakeHouseID with the right values. You can also use an Apache Livy endpoint, fo example for local tests: http://localhost:8998
    - **LIVY_REQUESTS_TIMEOUT**: The timeout in seconds for the Livy REST API requests
    - **LIVY_SESSION_NAME_PREFIX**: A prefix to use for session names. Example: MyApp-. A datetime will be appended to this prefix name
    - **LIVY_SPARK_CONF**: Optional custom Spark Configuration.
    For Microsoft Fabric only, an environmentID can be enabled using the Spark configuration ```'{"spark.fabric.environmentDetails" : "{\"id\": \"My_EnvironmentID\"}"}'```. You can get the environment ID from your Fabric workspace using the REST API: https://learn.microsoft.com/en-us/rest/api/fabric/environment/items/list-environments?tabs=HTTP. If no Environment_ID is specified, the session will default to the workspace's default environment on the default pool. For faster startup experience, sessions can use the Starter Pool, a medium-sized and prehydrated live pool that is automatically created for each workspace. More information for Starter Pools can be found here: https://learn.microsoft.com/en-us/fabric/data-engineering/configure-starter-pools
    - **LIVY_SPARK_DEPENDENCIES**: Optional, a comma separated absolute paths to the Python packages to be used in the Spark session. For example: *"abfss://...path-to.../Files/packages/mypackage-0.1.0-py3-none-any.whl"*
- Create groups on Django admin
    - Disable *AUTHENTICATION_BACKENDS = ("azure_auth.backends.AzureBackend",)* on the *settings.py** file
    - Create an admin account using ```python manage.py createsuperuser```
    - Apply migration ```python manage.py migrate``` and start the Django myapp ```python manage.py runserver localhost:5000```
    - Login using admin account from http://localhost:5000/admin
    - Create groups: Administrators, Editors, Viewers
    - Assign privileges to the groups
    - Enable back *AUTHENTICATION_BACKENDS = ("azure_auth.backends.AzureBackend",)* on the **settings.py** file, and restart the Django myapp

## Setup
Create a virtual environment and run:
```
pip install -r requirements.txt
```

## How to
**Already done, don't run**
```
django-admin startproject myapp
```

**Start the Django myapp**
```
cd myapp
python manage.py migrate
python manage.py runserver localhost:5000
```

## Important
- You need to manage the Fabric token expiration as well as the Livy session timeout (ttl, see Apache Livy reference bellow)
- If using Apache Livy 0.8, consider running some java_import before running any Spark code. See: [https://github.com/mounirbs/spark-livy/blob/main/python/livy/init_java_gateway.py#L11](https://github.com/mounirbs/spark-livy/blob/main/python/livy/init_java_gateway.py#L11) 
- Both ttl and idleTimeout seems not working properly in Fabric/Apache Livy. For Apache Livy, the binaries from https://livy.apache.org/download/ where used. Maybe the binaries are not reflecting the code on the Apache Livy master branch: [https://livy.incubator.apache.org/docs/latest/rest-api.html](https://github.com/apache/incubator-livy/blob/master/docs/rest-api.md). Without using these parameters, the session does not timeout.
- The code is not production ready, since it's not handling fully all the required exceptions. This is only a proof-of-concept!
- Installing packages when using Livy/Fabric:
    - Option 1 (*Fabric only*): On a Fabric environment and by specify the environment ID in the Spark configuration using the LIVY_SPARK_CONF environment variable, for example: 
      ```json
      {"spark.fabric.environmentDetails" : "{\"id\": \"My_EnvironmentID\"}"}
      ```
      *Starting sessions will not benefit from the fast startup experience of the Starter Pool, but will use the environment ID specified in the Spark configuration.*
    - Option 2 (*Fabric only*): Using a default Environment in the default Pool(Starter or Custom), you can add the Python packages to the environment using the Fabric UI. The packages will be available in all sessions that use this default environment. *Starting sessions will not benefit from the fast startup experience of the Starter Pool, but will use the environment ID specified in the Spark configuration.*
    - Option 3 (*Apache Livy/Fabric*): From the session: using the LIVY_SPARK_DEPENDENCIES environment variable (for the Livy pyFiles configuration), a list of comma separated absolute paths to the Python packages to be used in the Spark session.
    *Starting sessions will not benefit from the fast startup experience of the Starter Pool, but will use the environment ID specified in the Spark configuration.*
    - Option 4 (*Apache Livy/Fabric*): Directly from a session that benefits from the fast startup experience of the Starter Pool and by running:
      ```python
      import subprocess
      # If mypackage is on PyPi
      print(subprocess.check_output(["pip", "install", "mypackage"]))

      # If mypackage is on a wheel file on Fabric
      from notebookutils import mssparkutils  
      mssparkutils.fs.mount("abfss://...adfs_path../Files/packages/", "/packages")
      
      # This will return an absolute path to the mounted package folder (/synfs/notebook/xxx-yyy-zzz/packages)
      print(subprocess.check_output(["pip", "install", "/synfs/notebook/xxx-yyy-zzz/packages/mypackage-0.1.0-py3-none-any.whl"]))

      # Check installed packages      
      print(subprocess.check_output(["pip", "list"]).decode("utf-8"))
      ```
      *Note: installing packages directly on a session may install the package only on the driver and not on executors. The best way to install packages properly is to do it before the session starts (option 1,2,3)*

## Reference
- [https://learn.microsoft.com/en-us/fabric/data-engineering/get-started-api-livy-session](https://learn.microsoft.com/en-us/fabric/data-engineering/get-started-api-livy-session)
- [https://github.com/apache/incubator-livy/blob/master/docs/rest-api.md](https://github.com/apache/incubator-livy/blob/master/docs/rest-api.md)
- [https://livy.incubator.apache.org/docs/latest/rest-api.html](https://livy.incubator.apache.org/docs/latest/rest-api.html) (not up to date, idleTimeout is not there)
